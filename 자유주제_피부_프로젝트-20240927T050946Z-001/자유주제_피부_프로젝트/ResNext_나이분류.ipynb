{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rH1Fgh5tNzKZ",
    "outputId": "45d7dfdf-8f6f-4094-b62b-a880af45091b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data  나이분류.zip\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kpw8AIbgNuDi",
    "outputId": "0615f31e-9635-4080-b1b0-7f48ca524360"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config', '나이분류', '나이분류.zip', 'sample_data']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 압축 파일 경로\n",
    "zip_path = './나이분류.zip'\n",
    "\n",
    "# 압축을 풀 폴더 경로\n",
    "extract_path = './'\n",
    "\n",
    "# 압축 풀기\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# 압축이 풀린 파일 목록 확인\n",
    "os.listdir(extract_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfnQRFImO_gG",
    "outputId": "dd4824e5-8516-433d-830b-ec0e056b3132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 341556\n",
      "drwxr-xr-x 5 root root      4096 Sep 23 00:16 나이분류\n",
      "drwxr-xr-x 1 root root      4096 Sep 23 00:16 .\n",
      "drwxr-xr-x 1 root root      4096 Sep 23 00:00 ..\n",
      "drwxr-xr-x 4 root root      4096 Sep 19 13:25 .config\n",
      "drwxr-xr-x 1 root root      4096 Sep 19 13:25 sample_data\n",
      "-rw-r--r-- 1 root root 349728946 Sep 23 00:15 나이분류.zip\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mk0Eo5CSPo0T",
    "outputId": "ae7ad018-136f-43aa-a04f-260adb75bd39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/나이분류\n"
     ]
    }
   ],
   "source": [
    "cd 나이분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_RA_9qGTPxje",
    "outputId": "de723240-8d06-4dd5-bd15-8424d606965a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/나이분류'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PEbX03psPzDS",
    "outputId": "1623ed9a-ee79-47d3-e5fe-89107858e850"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n",
      "100%|██████████| 95.8M/95.8M [00:00<00:00, 155MB/s]\n",
      "Epoch 1/10: 100%|██████████| 431/431 [03:14<00:00,  2.22it/s, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0211076253804696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 431/431 [03:15<00:00,  2.20it/s, loss=0.976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.9755381224050322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 431/431 [03:14<00:00,  2.21it/s, loss=0.964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.9640533044553965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 431/431 [03:14<00:00,  2.21it/s, loss=0.965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.9648061083530328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 431/431 [03:14<00:00,  2.21it/s, loss=0.954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.9536708980591281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 431/431 [03:16<00:00,  2.19it/s, loss=0.966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.9662154117759032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 431/431 [03:14<00:00,  2.21it/s, loss=0.965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.964905275961086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 431/431 [03:14<00:00,  2.21it/s, loss=0.939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.9388825762022952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 431/431 [03:15<00:00,  2.20it/s, loss=0.941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.9413373774552843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 431/431 [03:15<00:00,  2.20it/s, loss=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.9297795649747561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:23<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 55.8584686774942%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # TQDM import\n",
    "\n",
    "# Custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.data = []\n",
    "\n",
    "        for label in range(len(self.classes)):\n",
    "            class_folder = os.path.join(root_dir, self.classes[label])\n",
    "            for filename in os.listdir(class_folder):\n",
    "                img_path = os.path.join(class_folder, filename)\n",
    "                self.data.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # 이미지 RGB로 변환\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 경로 및 배치 크기 설정\n",
    "data_dir = \".\"\n",
    "batch_size = 32\n",
    "\n",
    "# 데이터 증강 포함한 이미지 전처리\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),  # 랜덤 가로 뒤집기\n",
    "    T.RandomRotation(10),  # 랜덤 회전\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # 색상 변형\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 학습 및 검증 데이터셋 생성\n",
    "train_dataset = CustomDataset(os.path.join(data_dir, 'train'), transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = CustomDataset(os.path.join(data_dir, 'valid'), transform=transform)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 사전 학습된 resnext50_32x4d 모델 사용\n",
    "model = models.resnext50_32x4d(pretrained=True)\n",
    "\n",
    "# 전이 학습을 위해 일부 가중치 고정 (freeze)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 출력 레이어를 분류하려는 클래스 수에 맞게 수정\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.5),  # Dropout 추가\n",
    "    torch.nn.Linear(model.fc.in_features, len(train_dataset.classes))\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 학습률 스케줄러 추가 (학습률 점진적 감소)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# 학습 과정 (TQDM으로 진행 상황 시각화)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # TQDM으로 학습 진행 표시\n",
    "    train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for images, labels in train_loader_iter:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_loader_iter.set_postfix(loss=running_loss / len(train_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # 학습률 스케줄러 업데이트\n",
    "    scheduler.step()\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), 'resnext_model.pth')\n",
    "\n",
    "# 검증 평가\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    valid_loader_iter = tqdm(valid_loader, desc=\"Validating\")  # 검증도 TQDM으로 진행 상황 시각화\n",
    "    for images, labels in valid_loader_iter:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Validation Accuracy: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcDWWZ3bQPys",
    "outputId": "877692c5-4a6b-456c-d7bd-15b614dc40f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 431/431 [05:27<00:00,  1.32it/s, loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0825531928002696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 431/431 [05:26<00:00,  1.32it/s, loss=0.962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.96152822006053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 431/431 [05:26<00:00,  1.32it/s, loss=0.863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.8625074017905304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 431/431 [05:26<00:00,  1.32it/s, loss=0.795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.7950573925905604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 431/431 [05:26<00:00,  1.32it/s, loss=0.744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.744026968692129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 431/431 [05:26<00:00,  1.32it/s, loss=0.717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.7173273258447094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 431/431 [05:26<00:00,  1.32it/s, loss=0.698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.6979419654040768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 431/431 [05:26<00:00,  1.32it/s, loss=0.675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.6747959918599671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 431/431 [05:26<00:00,  1.32it/s, loss=0.662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.6621346672145507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 431/431 [05:26<00:00,  1.32it/s, loss=0.669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.66901260303234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 54/54 [00:23<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 69.3155452436195%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # TQDM import\n",
    "\n",
    "# Custom dataset\n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.data = []\n",
    "\n",
    "        for label in range(len(self.classes)):\n",
    "            class_folder = os.path.join(root_dir, self.classes[label])\n",
    "            for filename in os.listdir(class_folder):\n",
    "                img_path = os.path.join(class_folder, filename)\n",
    "                self.data.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # 이미지 RGB로 변환\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 경로 및 배치 크기 설정\n",
    "data_dir = \".\"\n",
    "batch_size = 32\n",
    "\n",
    "# 데이터 증강 포함한 이미지 전처리\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),  # 랜덤 가로 뒤집기\n",
    "    T.RandomRotation(10),  # 랜덤 회전\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # 색상 변형\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 학습 및 검증 데이터셋 생성\n",
    "train_dataset = CustomDataset(os.path.join(data_dir, 'train'), transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = CustomDataset(os.path.join(data_dir, 'valid'), transform=transform)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 사전 학습된 resnext50_32x4d 모델 사용\n",
    "model = models.resnext50_32x4d(pretrained=True)\n",
    "\n",
    "# 모든 레이어 학습 가능하도록 설정 (고정 부분 제거)\n",
    "# 이제 모든 파라미터가 학습에 참여함\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 출력 레이어를 분류하려는 클래스 수에 맞게 수정\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.5),  # Dropout 추가\n",
    "    torch.nn.Linear(model.fc.in_features, len(train_dataset.classes))\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.00005, momentum=0.9)\n",
    "\n",
    "# 학습률 스케줄러 추가 (학습률 점진적 감소)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# 학습 과정 (TQDM으로 진행 상황 시각화)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # TQDM으로 학습 진행 표시\n",
    "    train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for images, labels in train_loader_iter:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_loader_iter.set_postfix(loss=running_loss / len(train_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # 학습률 스케줄러 업데이트\n",
    "    scheduler.step()\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), 'resnext_model.pth')\n",
    "\n",
    "# 검증 평가\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    valid_loader_iter = tqdm(valid_loader, desc=\"Validating\")  # 검증도 TQDM으로 진행 상황 시각화\n",
    "    for images, labels in valid_loader_iter:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Validation Accuracy: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mgbE2dTqckRI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|████████████████████████████████████████████████████████| 863/863 [03:27<00:00,  4.16it/s, loss=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.7608831753460032\n",
      "Validation Accuracy: 69.77958236658932%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|████████████████████████████████████████████████████████| 863/863 [03:22<00:00,  4.26it/s, loss=0.626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Loss: 0.6255019191449652\n",
      "Validation Accuracy: 70.70765661252901%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|█████████████████████████████████████████████████████████| 863/863 [03:21<00:00,  4.27it/s, loss=0.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Loss: 0.5699071202984\n",
      "Validation Accuracy: 73.20185614849188%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|████████████████████████████████████████████████████████| 863/863 [03:24<00:00,  4.23it/s, loss=0.527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Loss: 0.5266329214961062\n",
      "Validation Accuracy: 72.62180974477958%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|████████████████████████████████████████████████████████| 863/863 [03:20<00:00,  4.31it/s, loss=0.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Loss: 0.495164231899025\n",
      "Validation Accuracy: 72.73781902552204%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|████████████████████████████████████████████████████████| 863/863 [03:20<00:00,  4.31it/s, loss=0.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Loss: 0.45606091718684755\n",
      "Validation Accuracy: 72.2737819025522%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|████████████████████████████████████████████████████████| 863/863 [03:20<00:00,  4.30it/s, loss=0.432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Loss: 0.432258178131398\n",
      "Validation Accuracy: 74.07192575406033%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████████████████████████████████████████████████████| 863/863 [03:24<00:00,  4.22it/s, loss=0.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Loss: 0.3998504717373489\n",
      "Validation Accuracy: 75.92807424593967%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|████████████████████████████████████████████████████████| 863/863 [03:21<00:00,  4.28it/s, loss=0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Loss: 0.3652687069559567\n",
      "Validation Accuracy: 72.62180974477958%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|███████████████████████████████████████████████████████| 863/863 [03:19<00:00,  4.32it/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Loss: 0.3436410233447477\n",
      "Validation Accuracy: 76.21809744779583%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|███████████████████████████████████████████████████████| 863/863 [03:18<00:00,  4.34it/s, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Loss: 0.2644685557886941\n",
      "Validation Accuracy: 75.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|███████████████████████████████████████████████████████| 863/863 [03:18<00:00,  4.36it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Loss: 0.22382964441813918\n",
      "Validation Accuracy: 74.65197215777262%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|███████████████████████████████████████████████████████| 863/863 [03:19<00:00,  4.33it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Loss: 0.2073700617208887\n",
      "Validation Accuracy: 73.78190255220417%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|███████████████████████████████████████████████████████| 863/863 [03:17<00:00,  4.36it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Loss: 0.18740442308013436\n",
      "Validation Accuracy: 74.94199535962878%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|████████████████████████████████████████████████████████| 863/863 [03:17<00:00,  4.37it/s, loss=0.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Loss: 0.1696912146303997\n",
      "Validation Accuracy: 73.72389791183295%\n",
      "Early stopping 적용\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # TQDM import\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "# Custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.data = []\n",
    "        \n",
    "        for label in range(len(self.classes)):\n",
    "            class_folder = os.path.join(root_dir, self.classes[label])\n",
    "            for filename in os.listdir(class_folder):\n",
    "                img_path = os.path.join(class_folder, filename)\n",
    "                self.data.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # 이미지 RGB로 변환\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 경로 및 배치 크기 설정\n",
    "data_dir = \".\"\n",
    "batch_size = 32\n",
    "\n",
    "# 데이터 증강 포함한 이미지 전처리\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(15),  # 더 큰 회전 각도\n",
    "    T.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 이미지 이동\n",
    "    T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),  # 색상 변형\n",
    "    T.GaussianBlur(kernel_size=3),  # Gaussian Blur 추가\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 학습 및 검증 데이터셋 생성\n",
    "train_dataset = CustomDataset(os.path.join(data_dir, 'train'), transform=transform)\n",
    "valid_dataset = CustomDataset(os.path.join(data_dir, 'valid'), transform=transform)\n",
    "\n",
    "# 각 클래스의 데이터 개수 계산 (Counter 사용)\n",
    "class_counts = Counter([label for _, label in train_dataset])\n",
    "\n",
    "# 클래스별로 가중치를 부여 (데이터 개수의 역수 사용)\n",
    "class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "\n",
    "# 각 샘플의 가중치를 리스트로 변환\n",
    "sample_weights = [class_weights[label] for _, label in train_dataset]\n",
    "\n",
    "# WeightedRandomSampler 생성\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights))\n",
    "\n",
    "# WeightedRandomSampler를 적용한 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 사전 학습된 resnext50_32x4d 모델 사용\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights\n",
    "\n",
    "weights = ResNeXt50_32X4D_Weights.DEFAULT\n",
    "model = models.resnext50_32x4d(weights=weights)\n",
    "\n",
    "# 모든 레이어 학습 가능하도록 설정 (전이 학습 제외)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 출력 레이어를 분류하려는 클래스 수에 맞게 수정 (드롭아웃 비율 0.3으로 설정)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.3),  # Dropout 비율을 0.3으로 조정\n",
    "    nn.Linear(model.fc.in_features, len(train_dataset.classes))\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Early Stopping 설정\n",
    "best_accuracy = 0\n",
    "patience = 5  # 개선되지 않는 에포크를 허용하는 최대 수\n",
    "counter = 0\n",
    "\n",
    "# 손실 함수 및 AdamW 옵티마이저 설정 (학습률 0.0001로 감소)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 학습률 스케줄러 추가 (학습률 점진적 감소, gamma=0.5, step_size=10)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# 학습 과정 (TQDM으로 진행 상황 시각화, 에포크 30)\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # TQDM으로 학습 진행 표시\n",
    "    train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, labels in train_loader_iter:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_loader_iter.set_postfix(loss=running_loss / len(train_loader))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "    \n",
    "    # 학습률 스케줄러 업데이트\n",
    "    scheduler.step()\n",
    "\n",
    "    # 검증 평가\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {accuracy}%')\n",
    "    \n",
    "    # Early Stopping 기준\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        counter = 0  # 성능 개선 시 카운터 초기화\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # 최적의 모델 저장\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping 적용\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|███████████████████████████████████████████████████████| 216/216 [07:55<00:00,  2.20s/it, loss=0.791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.7910682476229138\n",
      "Validation Accuracy: 69.60556844547564%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|███████████████████████████████████████████████████████| 216/216 [08:07<00:00,  2.26s/it, loss=0.593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Loss: 0.5930374179173399\n",
      "Validation Accuracy: 69.66357308584686%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|███████████████████████████████████████████████████████| 216/216 [07:33<00:00,  2.10s/it, loss=0.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Loss: 0.5385223522230431\n",
      "Validation Accuracy: 73.25986078886311%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|███████████████████████████████████████████████████████| 216/216 [05:21<00:00,  1.49s/it, loss=0.491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Loss: 0.491256192188572\n",
      "Validation Accuracy: 73.89791183294663%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|███████████████████████████████████████████████████████| 216/216 [05:55<00:00,  1.65s/it, loss=0.451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 0.4514921543498834\n",
      "Validation Accuracy: 73.60788863109049%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|███████████████████████████████████████████████████████| 216/216 [05:43<00:00,  1.59s/it, loss=0.415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Loss: 0.41532192744866564\n",
      "Validation Accuracy: 74.76798143851508%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|███████████████████████████████████████████████████████| 216/216 [05:42<00:00,  1.58s/it, loss=0.384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Loss: 0.3838025319079558\n",
      "Validation Accuracy: 73.78190255220417%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|███████████████████████████████████████████████████████| 216/216 [05:35<00:00,  1.55s/it, loss=0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Loss: 0.35245026151339215\n",
      "Validation Accuracy: 72.8538283062645%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|███████████████████████████████████████████████████████| 216/216 [06:05<00:00,  1.69s/it, loss=0.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Loss: 0.32595981367760235\n",
      "Validation Accuracy: 75.92807424593967%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████████████████████████████████████████████████| 216/216 [05:57<00:00,  1.65s/it, loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.28328480157587266\n",
      "Validation Accuracy: 75.34802784222738%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████████████████████████████████████████████████| 216/216 [04:30<00:00,  1.25s/it, loss=0.244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Loss: 0.24359282692549405\n",
      "Validation Accuracy: 76.16009280742459%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████████████████████████████████████████████████| 216/216 [03:33<00:00,  1.01it/s, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Loss: 0.19674221840169695\n",
      "Validation Accuracy: 75.63805104408353%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████████████████████████████████████████████████| 216/216 [03:31<00:00,  1.02it/s, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Loss: 0.18153227134435265\n",
      "Validation Accuracy: 75.52204176334106%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████████████████████████████████████████████████| 216/216 [03:32<00:00,  1.01it/s, loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Loss: 0.16116474863762656\n",
      "Validation Accuracy: 75.05800464037122%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████████████████████████████████████████████████| 216/216 [04:05<00:00,  1.14s/it, loss=0.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Loss: 0.1458194467963444\n",
      "Validation Accuracy: 76.62412993039443%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|███████████████████████████████████████████████████████| 216/216 [03:44<00:00,  1.04s/it, loss=0.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Loss: 0.12993475903446475\n",
      "Validation Accuracy: 74.0139211136891%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|███████████████████████████████████████████████████████| 216/216 [03:36<00:00,  1.00s/it, loss=0.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Loss: 0.12000551600768058\n",
      "Validation Accuracy: 74.53596287703016%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|███████████████████████████████████████████████████████| 216/216 [03:33<00:00,  1.01it/s, loss=0.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Loss: 0.1100142245111918\n",
      "Validation Accuracy: 74.88399071925754%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████████████████████████████████████████████████| 216/216 [03:35<00:00,  1.00it/s, loss=0.095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Loss: 0.09500225612255572\n",
      "Validation Accuracy: 75.75406032482599%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████████████████████████████████████████████████| 216/216 [03:23<00:00,  1.06it/s, loss=0.097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 0.09698562211081109\n",
      "Validation Accuracy: 75.63805104408353%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|█████████████████████████████████████████████████████| 216/216 [03:34<00:00,  1.01it/s, loss=0.0735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Loss: 0.07349310338893836\n",
      "Validation Accuracy: 75.52204176334106%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████████████████████████████████████████████████| 216/216 [03:30<00:00,  1.03it/s, loss=0.064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Loss: 0.06399960341406297\n",
      "Validation Accuracy: 75.34802784222738%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|█████████████████████████████████████████████████████| 216/216 [03:37<00:00,  1.01s/it, loss=0.0611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Loss: 0.06113607194964533\n",
      "Validation Accuracy: 76.62412993039443%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|█████████████████████████████████████████████████████| 216/216 [03:30<00:00,  1.03it/s, loss=0.0532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Loss: 0.05321656477516862\n",
      "Validation Accuracy: 75.52204176334106%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|█████████████████████████████████████████████████████| 216/216 [03:23<00:00,  1.06it/s, loss=0.0546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Loss: 0.0546055160467168\n",
      "Validation Accuracy: 75.5800464037123%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|█████████████████████████████████████████████████████| 216/216 [03:27<00:00,  1.04it/s, loss=0.0464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Loss: 0.046384206754198576\n",
      "Validation Accuracy: 76.56612529002321%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|█████████████████████████████████████████████████████| 216/216 [03:29<00:00,  1.03it/s, loss=0.0428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Loss: 0.04280897003115603\n",
      "Validation Accuracy: 75.29002320185614%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|█████████████████████████████████████████████████████| 216/216 [05:30<00:00,  1.53s/it, loss=0.0436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Loss: 0.043597310981971935\n",
      "Validation Accuracy: 75.63805104408353%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|█████████████████████████████████████████████████████| 216/216 [05:25<00:00,  1.51s/it, loss=0.0428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Loss: 0.042794372920912725\n",
      "Validation Accuracy: 75.9860788863109%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|█████████████████████████████████████████████████████| 216/216 [05:32<00:00,  1.54s/it, loss=0.0432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Loss: 0.043197707899337356\n",
      "Validation Accuracy: 75.23201856148492%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|█████████████████████████████████████████████████████| 216/216 [04:18<00:00,  1.20s/it, loss=0.0339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Loss: 0.03389811770412726\n",
      "Validation Accuracy: 75.87006960556845%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████████████████████████████████████████████████| 216/216 [03:37<00:00,  1.01s/it, loss=0.031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100, Loss: 0.0310487362321173\n",
      "Validation Accuracy: 76.10208816705337%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|█████████████████████████████████████████████████████| 216/216 [03:28<00:00,  1.04it/s, loss=0.0289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Loss: 0.028880091047203342\n",
      "Validation Accuracy: 76.16009280742459%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|█████████████████████████████████████████████████████| 216/216 [03:33<00:00,  1.01it/s, loss=0.0314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100, Loss: 0.03140771496044989\n",
      "Validation Accuracy: 74.88399071925754%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|█████████████████████████████████████████████████████| 216/216 [04:11<00:00,  1.16s/it, loss=0.0261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100, Loss: 0.026074306739987892\n",
      "Validation Accuracy: 75.34802784222738%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|█████████████████████████████████████████████████████| 216/216 [05:48<00:00,  1.61s/it, loss=0.0241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100, Loss: 0.024114842201082932\n",
      "Validation Accuracy: 75.87006960556845%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|█████████████████████████████████████████████████████| 216/216 [04:52<00:00,  1.35s/it, loss=0.0298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100, Loss: 0.02975374093943241\n",
      "Validation Accuracy: 75.52204176334106%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|██████████████████████████████████████████████████████| 216/216 [05:14<00:00,  1.45s/it, loss=0.022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100, Loss: 0.02202541112880378\n",
      "Validation Accuracy: 75.63805104408353%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|█████████████████████████████████████████████████████| 216/216 [05:31<00:00,  1.53s/it, loss=0.0238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100, Loss: 0.023849894059283002\n",
      "Validation Accuracy: 75.23201856148492%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|█████████████████████████████████████████████████████| 216/216 [05:21<00:00,  1.49s/it, loss=0.0234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Loss: 0.023425545124660455\n",
      "Validation Accuracy: 75.69605568445476%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|█████████████████████████████████████████████████████| 216/216 [05:21<00:00,  1.49s/it, loss=0.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100, Loss: 0.020722217628240794\n",
      "Validation Accuracy: 76.45011600928075%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|█████████████████████████████████████████████████████| 216/216 [05:06<00:00,  1.42s/it, loss=0.0221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100, Loss: 0.022119192573612695\n",
      "Validation Accuracy: 76.39211136890951%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|█████████████████████████████████████████████████████| 216/216 [04:51<00:00,  1.35s/it, loss=0.0206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100, Loss: 0.020644812814242432\n",
      "Validation Accuracy: 75.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|█████████████████████████████████████████████████████| 216/216 [03:44<00:00,  1.04s/it, loss=0.0196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100, Loss: 0.019602143966444094\n",
      "Validation Accuracy: 75.92807424593967%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|█████████████████████████████████████████████████████| 216/216 [05:27<00:00,  1.51s/it, loss=0.0164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100, Loss: 0.01635756100491832\n",
      "Validation Accuracy: 74.82598607888632%\n",
      "Early stopping 적용\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # TQDM import\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "# Custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.data = []\n",
    "        \n",
    "        for label in range(len(self.classes)):\n",
    "            class_folder = os.path.join(root_dir, self.classes[label])\n",
    "            for filename in os.listdir(class_folder):\n",
    "                img_path = os.path.join(class_folder, filename)\n",
    "                self.data.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # 이미지 RGB로 변환\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 경로 및 배치 크기 설정\n",
    "data_dir = \".\"\n",
    "batch_size = 64\n",
    "\n",
    "# 데이터 증강 포함한 이미지 전처리\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(15),  # 더 큰 회전 각도\n",
    "    T.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 이미지 이동\n",
    "    T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),  # 색상 변형\n",
    "    T.GaussianBlur(kernel_size=3),  # Gaussian Blur 추가\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 학습 및 검증 데이터셋 생성\n",
    "train_dataset = CustomDataset(os.path.join(data_dir, 'train'), transform=transform)\n",
    "valid_dataset = CustomDataset(os.path.join(data_dir, 'valid'), transform=transform)\n",
    "\n",
    "# 각 클래스의 데이터 개수 계산 (Counter 사용)\n",
    "class_counts = Counter([label for _, label in train_dataset])\n",
    "\n",
    "# 클래스별로 가중치를 부여 (데이터 개수의 역수 사용)\n",
    "class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "\n",
    "# 각 샘플의 가중치를 리스트로 변환\n",
    "sample_weights = [class_weights[label] for _, label in train_dataset]\n",
    "\n",
    "# WeightedRandomSampler 생성\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights))\n",
    "\n",
    "# WeightedRandomSampler를 적용한 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 사전 학습된 resnext50_32x4d 모델 사용\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights\n",
    "\n",
    "weights = ResNeXt50_32X4D_Weights.DEFAULT\n",
    "model = models.resnext50_32x4d(weights=weights)\n",
    "\n",
    "# 모든 레이어 학습 가능하도록 설정 (전이 학습 제외)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 출력 레이어를 분류하려는 클래스 수에 맞게 수정 (드롭아웃 비율 0.3으로 설정)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.3),  # Dropout 비율을 0.3으로 조정\n",
    "    nn.Linear(model.fc.in_features, len(train_dataset.classes))\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Early Stopping 설정\n",
    "best_accuracy = 0\n",
    "patience = 30  # 개선되지 않는 에포크를 허용하는 최대 수\n",
    "counter = 0\n",
    "\n",
    "# 손실 함수 및 AdamW 옵티마이저 설정 (학습률 0.0001로 감소)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 학습률 스케줄러 추가 (학습률 점진적 감소, gamma=0.5, step_size=10)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# 학습 과정 (TQDM으로 진행 상황 시각화, 에포크 30)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # TQDM으로 학습 진행 표시\n",
    "    train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, labels in train_loader_iter:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_loader_iter.set_postfix(loss=running_loss / len(train_loader))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "    \n",
    "    # 학습률 스케줄러 업데이트\n",
    "    scheduler.step()\n",
    "\n",
    "    # 검증 평가\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {accuracy}%')\n",
    "    \n",
    "    # Early Stopping 기준\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        counter = 0  # 성능 개선 시 카운터 초기화\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # 최적의 모델 저장\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping 적용\")\n",
    "            break\n",
    "\n",
    "# 베스트 모델의 혼동 행렬 시각화 및 성능 출력\n",
    "if best_cm is not None:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(best_cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Best Model Confusion Matrix\\nValidation Accuracy: {best_accuracy:.3f}%')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
